---
title: Keynote Speakers
nav: true
---

# Keynote Speakers


---

## üó£Ô∏è Speaker 1: Dr. Shruti Agarwal  

<div style="display:flex; align-items:flex-start; gap:20px;">

  <div style="flex:0 0 180px;">
    {% include figure.html img="IMG_0655.png" alt="Photo of Dr. Shruti Agarwal" caption="" width="100%" %}
  </div>

  <div style="flex:1;">


### **Bio**  
Shruti Agarwal, is a research scientist at Adobe‚Äôs Content Authenticity Initiative team. Her research focuses on building tools for multimedia forensics, content authenticity, and content provenance. Before joining Adobe, she was a postdoc at CSAIL MIT and a lecturer for the Computer Vision course in the MIDS program at Berkeley School of Information. She Ô¨Ånished her PhD from UC Berkeley under the guidance of Prof. Hany Farid in his world-leading lab on media forensics. During her PhD, she developed semantic tools using soft biometrics for person-speciÔ¨Åc deepfake detection. Currently, her research has focused on robust watermarking and content attribution. Her research is regularly published in top-tiered Computer Vision conferences and workshops. She has also been part of the organizing committee of: 1) CVPR workshop on Media Forensics in 2023 and 2024, 2) ICCV workshop APAI in 2025, 3) General Chair of ACM IH&MMSec 2025. She has also been a keynote speaker for CVPR workshop on Media Forensics 2022.

### **Talk Title**  
**Synthetic Data Attribution ‚Äãvia ‚ÄãWatermarking**

### **Abstract**  
Text-to-image foundation models, propelled by large-scale diffusion architectures, have demonstrated unprecedented success in generating high-fidelity, complex visual content from natural language descriptions. This progress has brought generative AI to the forefront of creative industries. However, this power introduces a critical challenge in proactive attribution: the need to embed imperceptible, robust watermarks into generated content to verify ownership and trace provenance. This task is fundamental to protecting the intellectual property of artists and creators whose unique styles and concepts are used to train these models. In this talk, I will present the progress on our recent work on ‚Äúproactive‚Äù attribution, embedding imperceptible watermarks directly into the pixel space of images. These methods train the diffusion model to preserve these spatial watermarks, allowing a decoder to later detect them in generated images and establish a causal connection to the original training examples that contribute to the generations. Such invention is effective for the general concept attribution tasks.

  </div>

</div>
---

<!-- ## üó£Ô∏è Speaker 2: ... 
{% include figure.html img="speaker2.jpg" alt="Photo of ..." caption="" width="40%" %}

<!-- **Affiliation:** Institute of Computational Thinking  
**Website:** https://example.org -->

<!-- ### **Bio**  

### **Talk Title**  
**...**

### **Abstract**  
...

--- --> -->