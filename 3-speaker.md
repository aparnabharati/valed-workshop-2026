---
title: Keynote Speakers
nav: true
---

# Keynote Speakers


---

## üó£Ô∏è Speaker 1: Dr. Shruti Agarwal  


<div style="display:flex; align-items:flex-start; gap:20px;">

<div style="flex:0 0 180px;">
{% include figure.html img="IMG_0655.png" alt="Photo of Dr. Shruti Agarwal" caption="" width="100%" %}
</div>

<div style="flex:1;" markdown="1">

### *Bio*
Shruti Agarwal, is a research scientist at Adobe‚Äôs Content Authenticity Initiative team. Her research focuses on building tools for multimedia forensics, content authenticity, and content provenance. Before joining Adobe, she was a postdoc at CSAIL MIT and a lecturer for the Computer Vision course in the MIDS program at Berkeley School of Information. She Ô¨Ånished her PhD from UC Berkeley under the guidance of Prof. Hany Farid in his world-leading lab on media forensics. During her PhD, she developed semantic tools using soft biometrics for person-speciÔ¨Åc deepfake detection. Currently, her research has focused on robust watermarking and content attribution. Her research is regularly published in top-tiered Computer Vision conferences and workshops. She has also been part of the organizing committee of: 1) CVPR workshop on Media Forensics in 2023 and 2024, 2) ICCV workshop APAI in 2025, 3) General Chair of ACM IH&MMSec 2025. She has also been a keynote speaker for CVPR workshop on Media Forensics 2022.

*Talk Title*: **Synthetic Data Attribution ‚Äãvia ‚ÄãWatermarking**

*Abstract*: Text-to-image foundation models, propelled by large-scale diffusion architectures, have demonstrated unprecedented success in generating high-fidelity, complex visual content from natural language descriptions. This progress has brought generative AI to the forefront of creative industries. However, this power introduces a critical challenge in proactive attribution: the need to embed imperceptible, robust watermarks into generated content to verify ownership and trace provenance. This task is fundamental to protecting the intellectual property of artists and creators whose unique styles and concepts are used to train these models. In this talk, I will present the progress on our recent work on ‚Äúproactive‚Äù attribution, embedding imperceptible watermarks directly into the pixel space of images. These methods train the diffusion model to preserve these spatial watermarks, allowing a decoder to later detect them in generated images and establish a causal connection to the original training examples that contribute to the generations. Such invention is effective for the general concept attribution tasks.

</div>

</div>

---


## üó£Ô∏è Speaker 2: Dr. Vishal M. Patel  


<div style="display:flex; align-items:flex-start; gap:20px;">

<div style="flex:0 0 180px;">
{% include figure.html img="VP_JHU.png" alt="Photo of Dr. Vishal M. Patel" caption="" width="100%" %}
</div>

<div style="flex:1;" markdown="1">

### *Bio*
Vishal M. Patel is an Associate Professor in the Department of Electrical and Computer Engineering (ECE) at Johns Hopkins University. His research focuses on computer vision, machine learning, image processing, medical image analysis, and biometrics. He has received a number of awards including the 2021 IEEE Signal Processing Society (SPS) Pierre-Simon Laplace Early Career Technical Achievement Award, the 2021 NSF CAREER Award, the 2021 IAPR Young Biometrics Investigator Award (YBIA), the 2016 ONR Young Investigator Award, and the 2016 Jimmy Lin Award for Invention. Patel serves as an associate editor for the IEEE Transactions on Pattern Analysis and Machine Intelligence journal and IEEE Transactions on Biometrics, Behavior, and Identity Science. He also chairs the conference subcommittee of IAPR Technical Committee on Biometrics (TC4). He is a fellow of the IAPR and a senior member of AAAI. 

*Talk Title*: **Unlearning for Safer Generative AI: From Concept Erasure to Model Accountability**

*Abstract*: Ensuring safety, accountability, and responsible use of modern generative AI models requires not only detecting harmful outputs, but also actively removing or unlearning undesirable concepts from the underlying models. In this talk, I will present our recent progress on robust concept erasure and unlearning, highlighting methods that allow generative models to ‚Äúforget‚Äù unsafe, copyrighted, or ethically sensitive concepts while preserving their overall utility. I will discuss classifier-guided and black-box erasure techniques, which modify only the text embeddings at inference time and enable user-controlled filtering without accessing model weights. I will then describe STEREO, our two-stage adversarially robust framework that overcomes the utility‚Äìsafety trade-off common in existing concept-erasure methods. The talk will primarily focus on unlearning as a key pillar for building safer, legally compliant, and ethically aligned generative AI systems.

</div>

</div>

---